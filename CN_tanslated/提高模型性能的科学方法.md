# 提高模型性能的科学方法

本文档的目的，机器学习开发的最终目标是最大限度地提高已部署模型的效用。尽管开发过程的许多方面在应用程序之间有所不同（例如时间长度、可用计算资源、模型类型），但我们通常可以对任何问题使用相同的基本步骤和原则。

我们下面的指南做出了以下假设：

- 已经有一个完全运行的训练管道以及获得合理结果的配置。

- 有足够的计算资源来进行有意义的调优实验，并至少能够并行运行几个训练工作。

## 增量调优策略

**摘要**：从简单的配置开始，逐步进行改进，同时建立对问题的洞察力。确保任何改进基于强有力的证据，以避免增加不必要的复杂性。

- 我们的最终目标是找到一种配置，使我们的模型性能最大化。

- 在某些情况下，我们的目标将是在一个固定的截止日期前最大限度地提高模型的水平（例如提交给一个比赛）。

- 在其他情况下，我们希望无限期地改进模型（例如，不断地改进生产中使用的模型）。

- 原则上，我们可以通过使用一种算法来自动搜索整个可能的配置空间，从而最大限度地提高性能，但这并不是一个实用的选择。

- 可能的配置空间非常大，目前还没有任何足够成熟的算法可以在没有人类指导的情况下有效地搜索这个空间。

- 大多数自动搜索算法依赖于手工设计的搜索空间，该空间定义了要搜索的配置集，并且这些搜索空间可能相当重要。

- 将性能最大化的最有效方法是从简单的配置开始，逐步增加功能并进行改进，同时建立对问题的洞察力。

- 我们在每一轮调整中使用自动搜索算法，并随着我们理解的加深而不断地更新我们的搜索空间。

- 随着我们的探索，我们自然会发现越来越好的配置，因此我们的 "最佳 "模型将不断改进。

- 当我们更新我们的最佳配置时，我们称之为推进（可能是也可能不是对应于一个生产模型的实际推进）。

- 对于每一次推进，我们必须确保这种变化是基于强有力的证据--而不是基于随机更新的配置--这样我们就不会给训练管道增加不必要的复杂性。

在高层次上，我们的增量调整策略包括重复以下四个步骤：

1. 为下一轮的实验确定一个适当范围的目标。

2. 设计并运行一套实验，使之在这个目标上取得进展。

3. 从结果中学习我们能做的。

4. 考虑是否推出新的最佳配置。

本节的其余部分将更详细地考虑这一策略。

## 探索vs开发

**摘要**：大多数时候，我们的首要目标是深入了解问题。

- 尽管人们可能会认为我们会花大部分时间试图在验证集上实现性能最大化，但在实践中，我们花了大部分时间试图深入了解问题，而相对来说很少有时间深入地关注验证错误。

- 换句话说，我们把大部分时间花在 "探索 "上，只有少量的时间花在 "开发 "上。

- 从长远来看，如果我们想最大限度地提高我们的最终效用，了解问题是至关重要的。将洞察力的优先级设置高于短期收益的优先级可以帮助我们。

- 避免推进不必要的变化，这些变化恰好能存在于表现良好的运行中，只是由于历史上的偶然。

- 识别哪些超参数对验证误差最敏感，哪些超参数相互影响最大，因此需要一起重新调整，哪些超参数对其他变化相对不敏感，因此可以在未来的实验中固定下来。

- 建议尝试潜在的新特征，如新的正则器，如果存在过拟合的情形。

- 识别那些没有帮助的功能，这样可以删除，减少未来实验的复杂性。

- 识别超参数调整带来的改进何时可能达到饱和。

- 围绕最优值缩小我们的搜索空间，以提高调参效率。

- 当我们最终准备好‘贪婪’的时候，我们可以纯粹地关注验证误差，即使实验没有最大地信息化利用调参问题的结构。

## 为下一轮的实验选择目标

**摘要**：每一轮的实验都应该有一个明确的目标，而且范围要足够窄，使实验能够真正朝着目标取得进展。

- 每一轮实验都应该有一个明确的目标，而且范围要足够窄，使实验能够真正朝着目标前进：如果我们试图一次增加多个功能或回答多个问题，我们可能无法区分对结果的单独影响。

- 目标示例包括：

- 尝试对管道进行潜在的改进（例如，新的正则器、预处理选择等）。

- 了解特定模型超参数（如激活函数）的影响

- 贪婪地最大化验证错误。

## 设计下一轮的实验

**摘要**：确定哪些超参数是实验目标的科学超参数、滋扰超参数和固定超参数。建立一个研究序列来比较科学超参数的不同值，同时对滋扰超参数进行优化。选择干扰性超参数的搜索空间，以平衡资源成本和科学价值。

### 识别科学的、讨厌的和固定的超参数

- 对于一个给定的目标，所有的超参数将是科学超参数、滋生超参数或固定超参数。

- 科学的超参数是那些我们试图测量其对模型性能的影响。

- 滋扰超参数是那些需要优化的参数，以便公平地比较科学超参数的不同值。这类似于统计学中的滋扰参数的概念。

- 固定的超参数将在本轮实验中固定其数值。这些超参数的值在比较科学超参数的不同值时不需要（或我们不希望它们）改变。

- 通过为一组实验固定某些超参数，我们必须接受从实验中得出的结论可能对固定超参数的其他设置无效。换句话说，固定的超参数为我们从实验中得出的任何结论创造了警告条件。

- 例如，如果我们的目标是 "确定具有更多隐藏层的模型是否会减少验证误差"，那么隐藏层的数量就是一个科学的超参数。

- 学习率是一个麻烦的超参数，因为我们只有在为每个层数分别调整学习率的情况下，才能公平地比较具有不同层数的模型（最佳学习率一般取决于模型结构）。

- 如果我们在之前的实验中确定激活函数的最佳选择对模型深度不敏感，或者我们愿意限制我们对隐藏层数量的结论，只包括激活函数的具体选择，那么激活函数可以是一个固定的超参数。另外，如果我们准备为每一个隐藏层的数量分别进行调整，它也可以是一个讨厌的参数。

- 一个特定的超参数是科学超参数、滋扰超参数还是固定超参数，并不是该超参数所固有的，而是根据实验目标而变化。

- 例如，激活函数的选择可以是一个科学的超参数（对于我们的问题，ReLU或tanh是更好的选择吗？），一个滋扰超参数（当我们允许几个不同的可能激活函数时，最好的5层模型是否比最好的6层模型更好？），或一个固定的超参数（对于ReLU网，在特定位置添加批量归一化是否有帮助？）

- 在设计新一轮的实验时，我们首先为我们的实验目标确定科学的超参数。

- 在这个阶段，我们认为所有其他的超参数都是滋扰性超参数。

- 接下来，我们把一些滋生的超参数转换成固定的超参数。

- 在资源无限的情况下，我们会把所有非科学的超参数作为滋生性超参数，这样我们从实验中得出的结论就不会受到固定超参数值的告诫。

- 然而，我们试图调整的滋扰性超参数越多，就越有可能在每一个科学超参数的设置上不能充分地调整，最终从实验中得出错误的结论。

- 如下所述，我们可以通过增加计算预算来应对这一风险，但往往我们的最大资源预算少于在所有非科学的超参数上进行调整所需的资源。

- 当我们判断固定超参数引入的注意事项比把它作为一个讨厌的超参数的成本低时，我们会选择把一个讨厌的超参数转换成一个固定的超参数。

- 一个给定的滋扰超参数与科学超参数的相互作用越大，固定其数值的破坏性就越大。例如，权重衰减强度的最佳值通常取决于模型的大小，因此，假设一个单一的权重衰减的特定值来比较不同的模型大小，不会有很好的洞察力。

- 尽管我们给每个超参数分配的类型取决于实验目标，但对于某些类别的超参数，我们有以下经验法则。

- 在各种优化器的超参数中（如学习率、动量、学习率计划参数、Adam betas等），至少有一些会成为讨厌的超参数，因为它们往往与其他变化的互动最多。

- 它们很少是科学的超参数，因为像 "当前管道的最佳学习率是多少？"这样的目标并不能提供太多的洞察力--无论如何，最佳设置很容易随着下一个管道的改变而改变。

- 尽管由于资源的限制，或者当我们有特别有力的证据证明它们与科学参数没有相互作用时，我们可能会偶尔固定其中的一些参数，但一般来说，我们应该假定优化器的超参数必须单独调整，以便在科学超参数的不同设置之间进行公平的比较，因此不应该被固定。

- 此外，我们没有先验的理由去选择一个优化器的超参数值而不是另一个（例如，它们通常不会以任何方式影响正向传递或梯度的计算成本）。

- 相比之下，优化器的选择通常是科学超参数或固定超参数。

- 如果我们的实验目标涉及在两个或多个不同的优化器之间进行公平的比较（例如 "确定哪个优化器在给定的步骤数中产生最低的验证误差"），它就是一个科学的超参数。

- 另外，我们可能出于各种原因将其作为一个固定的超参数，包括：（1）先前的实验使我们相信，对我们的问题来说，最好的优化器对当前的科学超参数不敏感；和/或（2）我们更愿意使用这个优化器来比较科学超参数的值，因为它的训练曲线更容易推理；和/或（3）我们更愿意使用这个优化器，因为它比替代方案使用的内存少。

- 正则化技术引入的超参数通常是滋扰超参数，但我们是否包含正则化技术则是一个科学或固定的超参数。

- 例如，辍学会增加代码的复杂性，因此在决定是否包括它时，我们会将 "无辍学 "与 "辍学 "作为一个科学的超参数，而将辍学率作为一个讨厌的超参数。

- 如果我们决定在这个实验的基础上将辍学加入我们的管道，那么在未来的实验中，辍学率将是一个讨厌的超参数。

- 架构超参数通常是科学的或固定的超参数，因为架构变化会影响服务和训练成本、延迟和内存要求。

- 例如，层数通常是一个科学的或固定的超参数，因为它往往会对训练速度和内存使用产生巨大的影响。

- 在某些情况下，滋生参数和固定超参数的集合将取决于科学超参数的值。

- 例如，假设我们试图确定在Nesterov动量和Adam中哪一个优化器会导致最低的验证误差。科学的超参数是优化器，其取值为{"Nesterov_momentum", "Adam"}。优化器="Nesterov_momentum "的值引入了滋扰/固定的超参数{learning_rate, momentum}，但优化器="Adam "的值引入了滋扰/固定的超参数{learning_rate, beta1, beta2, epsilon}。

- 只有在科学超参数的某些数值下才会出现的超参数被称为条件性超参数。

- 我们不应该仅仅因为两个条件超参数有相同的名字就认为它们是相同的 在上面的例子中，名为learning_rate的条件超参数对于optimizer="Nesterov_momentum "和optimizer="Adam "是不同的超参数。它在两种算法中的作用相似（虽然不完全相同），但在每个优化器中运行良好的数值范围通常有几个数量级的差异。

### 创建一套研究报告

- 一旦我们确定了科学和滋生的超参数，我们就会设计一个 "研究 "或一系列研究，以便朝着实验目标取得进展。

- 一项研究指定了一组为后续分析而运行的超参数配置。每个配置被称为一个 "试验"。

- 创建一个研究通常包括选择将在不同试验中变化的超参数，选择这些超参数的取值（"搜索空间"），选择试验的数量，并选择一个自动搜索算法来从搜索空间中抽取该数量的试验。另外，我们也可以通过手动指定超参数配置的集合来创建一个研究。

- 研究的目的是用不同的科学超参数值运行管道，同时 "优化掉"（或 "优化"）滋生的超参数，以便科学超参数的不同值之间的比较尽可能公平。

- 在最简单的情况下，我们会对科学参数的每个配置进行单独的研究，每个研究对滋生超参数进行调整。

- 例如，如果我们的目标是在Nesterov动量和Adam中选择最佳优化器，我们可以创建一个研究，其中优化器="Nesterov_momentum"，滋扰超参数为{学习率，动量}，另一个研究中优化器="Adam"，滋扰超参数为{学习率，β1，β2，ε}。我们将通过从每个研究中选择表现最好的试验来比较这两个优化器。

- 我们可以使用任何无梯度的优化算法，包括贝叶斯优化或进化算法等方法，对滋生的超参数进行优化，不过我们更倾向于在探索阶段使用准随机搜索进行调优，因为它在这种情况下有各种优势。在探索结束后，如果有最先进的贝叶斯优化软件，这就是我们的首选。

- 在更复杂的情况下，我们想比较大量的科学超参数值，而做那么多独立的研究是不现实的，我们可以把科学参数和干扰超参数列入同一个搜索空间，并使用搜索算法在一个研究中对科学和干扰超参数的值进行抽样。

- 当采取这种方法时，条件性超参数会引起问题，因为除非滋扰性超参数集对科学超参数的所有值都是相同的，否则很难指定一个搜索空间。

- 在这种情况下，我们更倾向于使用准随机搜索而不是更复杂的黑箱优化工具，因为它能确保我们获得相对统一的科学超参数值的抽样。不管是哪种搜索算法，我们都需要以某种方式确保它能均匀地搜索科学参数。

### 在内容丰富的实验和负担得起的实验之间取得平衡

- 在设计一项研究或一系列研究时，我们需要分配有限的预算，以充分实现以下三个愿望。
1. 比较足够多的科学超参数的不同值。

2. 在足够大的搜索空间内调整滋生的超参数。

3. 对滋生超参数的搜索空间进行足够密集的采样。
- 我们越能实现这三个愿望，就能从我们的实验中获得更多的洞察力。

- 尽可能多地比较科学超参数的值，扩大了我们从实验中获得的洞察力的范围。

- 包括尽可能多的滋扰超参数，并允许每个滋扰超参数在尽可能大的范围内变化，这增加了我们的信心，即对于科学超参数的每个配置，在搜索空间中存在一个 "好 "的滋扰超参数值。

- 否则，我们可能会在科学超参数的值之间进行不公平的比较，因为我们没有搜索滋生参数空间的可能区域，在那里科学参数的某些值可能存在更好的值。

- 尽可能密集地对滋扰超参数的搜索空间进行采样，增加了我们的信心，即任何恰好存在于我们搜索空间中的滋扰超参数的良好设置都会被搜索程序发现。

- 否则，我们可能会在科学参数的数值之间进行不公平的比较，因为某些数值在扰动超参数的抽样中变得更加幸运。

- 不幸的是，这三个维度中的任何一个的改进都需要增加试验的数量，从而增加资源成本，或者找到一种方法来节省其他维度的资源。

- 每个问题都有自己的特异性和计算限制，所以如何在这三个方面分配资源需要一定程度的领域知识。

- 在运行一项研究后，我们总是试图了解该研究是否对滋扰性超参数进行了足够好的调整（即对足够大的空间进行了足够广泛的搜索），以公平地比较科学的超参数（如下文更详细地描述）。

## 从实验结果中提取洞察力

**摘要**：除了努力实现每组实验的原始科学目标外，还要通过附加问题的检查表，如果发现问题，要修改实验并重新运行。

- 最终，每组实验都有一个具体的目标，我们要评估实验为实现这一目标提供的证据。

- 然而，如果我们提出正确的问题，我们往往会发现在一组特定的实验能够朝着最初的目标取得很大进展之前需要纠正的问题。

- 如果我们不问这些问题，我们可能会得出不正确的结论。

- 由于运行实验可能是昂贵的，我们也想借此机会从每组实验中提取其他有用的见解，即使这些见解与当前的目标没有直接关系。

- 在分析一组特定的实验，使其向最初的目标迈进之前，我们应该问自己以下的附加问题：

- 搜索空间是否足够大？

- 如果一项研究的最佳点在一个或多个维度上接近搜索空间的边界，那么搜索的范围可能不够大。在这种情况下，我们应该用扩大的搜索空间再进行一次研究。

- 我们是否从搜索空间中抽出了足够的点？

- 如果不是，就多跑点，或者在调校目标上不那么雄心勃勃。

- 在每项研究中，有多大比例的试验是不可行的（即试验出现分歧，得到非常糟糕的损失值，或者因为违反了一些隐含的约束条件而根本无法运行）？

- 当研究中很大一部分点是不可行的，我们应该尝试调整搜索空间，以避免对这些点进行采样，这有时需要重新参数化搜索空间。

- 在某些情况下，大量的不可行点可能表明训练代码中存在错误。

- 该模型是否表现出优化问题？

- 我们能从最佳试验的训练曲线中学到什么？

- 例如，最佳试验的训练曲线是否与有问题的过度拟合一致？

- 如有必要，根据上述问题的答案，完善最近的研究（或一组研究），以改进搜索空间和/或对更多的试验进行抽样，或采取一些其他纠正措施。

- 一旦我们回答了上述问题，我们就可以继续评估实验为实现我们最初的目标所提供的证据（例如，评估一个变化是否有用）。

### 识别不好的搜索空间边界

- 如果一个搜索空间中的最佳采样点接近其边界，那么这个搜索空间就是可疑的。如果我们向那个方向扩大搜索范围，我们可能会找到一个更好的点。

- 为了检查搜索空间的边界，我们喜欢将完成的试验绘制在我们称之为基本的超参数轴图上，我们将验证目标值与其中一个超参数（例如学习率）进行对比。图上的每一个点都对应于一个试验。

- 每次试验的验证目标值通常应该是它在训练过程中取得的最佳值。

- 缺失图片

- 图1：不良搜索空间边界和可接受的搜索空间边界的例子。

- 图1中的图显示了错误率（越低越好）与初始学习率的关系。

- 如果最佳点聚集在搜索空间的边缘（在某些维度上），那么搜索空间的边界可能需要扩大，直到最佳观察点不再靠近边界。

- 通常，一项研究会包括 "不可行 "的试验，这些试验会出现分歧或得到非常糟糕的结果（在上图中用红色X标记）。

- 如果所有的试验对于学习率大于某个阈值的情况下都是不可行的，而如果表现最好的试验的学习率处于该区域的边缘，那么该模型可能存在稳定性问题，使其无法获得更高的学习率。

### 在搜索空间中没有足够的采样点

- 一般来说，要知道搜索空间的采样是否足够密集是非常困难的。

- 进行更多的试验当然更好，但也有明显的代价。

- 由于很难知道我们什么时候采样够了，我们通常会对我们能承受的东西进行采样，并试图从反复观察各种超参数轴图中校准我们的直觉信心，并试图了解有多少点是在搜索空间的 "好 "区域。

### 检查训练曲线

**摘要**：检查训练曲线是识别常见故障模式的一个简单方法，可以帮助我们确定下一步要采取的行动的优先次序。

- 虽然在很多情况下，我们实验的主要目标只需要考虑每一次试验的验证误差，但当把每一次试验简化为一个单一的数字时，我们必须小心，因为它可能会隐藏表面之下的重要细节。

- 对于每项研究，我们总是看至少最好的几项试验的训练曲线（训练误差和验证误差与训练步骤在训练期间的关系图）。

- 即使这对解决主要的实验目标没有必要，检查训练曲线也是识别常见故障模式的一个简单方法，可以帮助我们确定下一步要采取的行动的优先次序。

- 在研究训练曲线时，我们对以下问题感兴趣。

- 是否有任何试验表现出有问题的过度拟合？

- 当验证误差在训练过程中的某个时刻开始增加时，就会出现有问题的过拟合。

- 在实验环境中，我们通过为每个科学超参数的设置选择 "最佳 "试验来优化干扰超参数，我们应该至少在对应于我们正在比较的科学超参数设置的每个最佳试验中检查有问题的过度拟合。

- 如果任何一个最佳试验表现出有问题的过拟合，我们通常要用额外的正则化技术重新进行试验，和/或在比较科学超参数的值之前更好地调整现有的正则化参数。

- 如果科学的超参数包括正则化参数，这可能就不适用了，因为那样的话，如果这些正则化参数的低强度设置导致了有问题的过拟合，也就不奇怪了。

- 减少过拟合通常是直接使用常见的正则化技术，这些技术增加了最小的代码复杂性或额外的计算（例如dropout、标签平滑、权重衰减），所以在下一轮实验中增加一个或多个这样的技术通常没什么大不了的。

- 例如，如果科学的超参数是 "隐藏层的数量"，而使用最大数量的隐藏层的最佳试验表现出有问题的过拟合，那么我们通常倾向于用额外的正则化来再次尝试，而不是立即选择较小数量的隐藏层。

- 即使没有一个 "最好的 "试验表现出有问题的过度拟合，但如果在任何试验中出现这种情况，仍然可能有问题。

- 选择最好的试验会抑制表现出有问题的过拟合的配置，而偏向于那些没有问题的配置。换句话说，它将有利于具有更多规范化的配置。

- 然而，任何使训练变得更糟糕的东西都可以作为正则器，即使它并不打算这样做。例如，选择一个较小的学习率可以通过阻碍优化过程来规范训练，但我们通常不希望以这种方式选择学习率。

- 因此，我们必须意识到，为每一个科学超参数设置的 "最佳 "试验可能被选择为有利于某些科学或干扰性超参数的 "坏 "值的方式。

- 在训练后期，训练或验证误差是否存在较高的步与步之间的差异？

- 如果是这样，这可能会干扰我们比较不同的科学超参数值的能力（因为每个试验都随机地在 "幸运 "或 "不幸运 "的步骤上结束），以及我们在生产中复制最佳试验结果的能力（因为生产模型可能不会像研究中那样在 "幸运 "的步骤上结束）。

- 最有可能导致步与步之间差异的原因是批次差异（从每批训练集中随机抽取例子），小的验证集，以及在训练后期使用过高的学习率。

- 可能的补救措施包括增加批次大小，获得更多的验证数据，使用学习率衰减，或使用Polyak平均法。

- 训练结束后，试验是否仍在改进？

- 如果是这样，这表明我们处于 "计算约束 "状态，我们可能会从增加训练步骤的数量或改变学习率计划中受益。

- 在最后的训练步骤之前，训练集和验证集的性能是否早已饱和？

- 如果是这样，这表明我们处于 "不受计算限制 "的状态，我们也许可以减少训练步骤的数量。

- 虽然我们不能一一列举，但还有许多其他的行为可以通过检查训练曲线而变得明显（例如，训练损失在训练过程中增加，通常表明训练管道中存在错误）。

### 用隔离图检测一个变化是否有用

缺少图片

图2：研究在ImageNet上训练的ResNet-50的最佳权重衰减值的隔离图。

- 通常，一组实验的目标是比较一个科学超参数的不同值。

- 例如，我们可能想确定导致最佳验证误差的权重衰减值。

- 隔离图是基本超参数轴图的一个特例。隔离图上的每一个点都对应于一些（或全部）干扰超参数的最佳试验的性能。

- 换句话说，我们在 "优化 "了讨厌的超参数之后，绘制了模型的性能。

- 隔离图使其更容易在科学超参数的不同值之间进行苹果对苹果的比较。

- 例如，图2显示了在ImageNet上训练的ResNet-50的特定配置下产生最佳验证性能的权重衰减值。

- 如果我们的目标是确定是否要加入权重衰减，那么我们将把这个图中的最佳点与没有权重衰减的基线进行比较。为了进行公平的比较，基线也应该对其学习率进行同样良好的调整。

- 当我们有由（准）随机搜索产生的数据，并考虑一个连续的超参数的隔离图时，我们可以通过对基本的超参数轴图的X轴值进行分桶，并在分桶定义的每个垂直切片中取最佳试验来近似隔离图。

### 自动生成通用的有用图谱

- 产生地块的努力越多，我们就越不可能尽可能多地去看它们，因此，我们有必要设置我们的基础设施，以自动产生尽可能多的地块。

- 至少，我们会对实验中改变的所有超参数自动生成基本的超参数轴图。

- 此外，我们为所有的试验自动生成训练曲线，并尽可能方便地找到每项研究中最好的几个试验并检查它们的训练曲线。

- 我们还可以添加许多其他潜在的绘图和可视化的东西，这些都是有用的。虽然上面描述的那些是一个很好的起点，但套用杰弗里-辛顿的话说，"每当你绘制新的东西，你就会学到新的东西。"

## 确定是否采用训练管道变化或超参数配置

**摘要**：当决定是否对我们的模型或训练程序进行改变或采用新的超参数配置前进时，我们需要意识到我们结果中不同的变化来源。

- 当我们试图改进我们的模型时，我们可能会观察到，与我们现有的配置相比，一个特定的候选变化最初取得了更好的验证误差，但在重复实验后发现，没有一致的优势。非正式地，我们可以把可能导致这种不一致结果的最重要的变化来源分为以下几大类：

- 训练程序方差、再训练方差或试验方差：我们在使用相同的超参数，但不同的随机种子的训练运行之间看到的差异。

- 例如，不同的随机初始化、训练数据洗牌、辍学掩码、数据增强操作的模式和并行算术操作的顺序，都是试验差异的潜在来源。

- 超参数搜索方差，或研究方差：由我们选择超参数的程序引起的结果差异。

- 例如，我们可能用一个特定的搜索空间运行同一个实验，但用两个不同的种子进行准随机搜索，最后选择不同的超参数值。

- 数据收集和抽样差异：任何一种随机分成训练、验证和测试数据的差异，或更普遍地由于训练数据生成过程而产生的差异。

- 使用严格的统计测试对有限验证集上估计的验证错误率进行比较是很好的，但往往仅试验方差就能在使用相同超参数设置的两个不同的训练模型之间产生统计上的显著差异。

- 在试图做出超越超参数空间中单个点的水平的结论时，我们最关心的是研究方差。

- 研究方差取决于试验的数量和搜索空间，我们已经看到了它大于试验方差的情况，以及它小得多的情况。

- 因此，在采用一个候选变化之前，考虑运行最佳试验N次，以描述运行-运行试验的差异。

- 通常情况下，我们可以在管道发生重大变化后只对试验差异进行重新定性，但在某些应用中，我们可能需要更新鲜的估计。

- 在其他应用中，表征试验方差的成本太高，不值得。

- 在一天结束时，虽然我们只想采用能产生真正改进的变化（包括新的超参数配置），但要求完全确定某种东西有帮助，也不是正确的答案。

- 因此，如果一个新的超参数点（或其他变化）得到了比基线更好的结果（尽可能考虑到新点和基线的再训练方差），那么我们可能应该把它作为新的基线，用于未来的比较。

- 然而，我们应该只采用那些产生的改进超过其增加的任何复杂性的变化。

## 探索结束后

**摘要**：一旦我们完成了对好的搜索空间的探索，并决定了哪些超参数甚至应该被调整，贝叶斯优化工具就是一个引人注目的选择。

- 在某些时候，我们的优先事项将从学习更多关于调谐问题转向产生一个单一的最佳配置来启动或以其他方式使用。

- 在这一点上，应该有一个精炼的搜索空间，它舒适地包含了最佳观察试验周围的局部区域，并且已经被充分地采样了。

- 我们的探索工作应该已经揭示了最基本的超参数调整（以及它们的合理范围），我们可以使用尽可能大的调整预算来构建一个搜索空间，以进行最终的自动调整研究。

- 由于我们不再关心最大化我们对调谐问题的洞察力，准随机搜索的许多优势不再适用，应该使用贝叶斯优化工具来自动寻找最佳超参数配置。

- 开源的Vizier实现了各种复杂的算法来调整ML模型，包括贝叶斯优化算法。

- 如果搜索空间包含大量的发散点（得到NaN训练损失的点，甚至是比平均值差很多标准差的训练损失的点），那么使用能正确处理发散试验的黑匣子优化工具是很重要的（关于处理这个问题的一个很好的方法，请看带有未知约束条件的贝叶斯优化）。开源的Vizier支持发散点，将试验标记为不可行，尽管它可能不会使用我们从Gelbart等人那里得到的首选方法，这取决于它的配置方式。

- 在这一点上，我们还应该考虑检查测试集的性能。

- 原则上，我们甚至可以将验证集折叠到训练集中，重新训练用贝叶斯优化法找到的最佳配置。然而，这只适合于未来不会有这种特定工作量的发射（例如，一次性的Kaggle比赛）。
